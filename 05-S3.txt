S3 is an object based storage system
S3 bucket is a container for objects; AWS fully managed, fully scalable
you can access the bucket object by URL (https), the URL format maybe
https://<bucket>.s3.<aws-region>.amazonaws.com/<key>
https://s3.<aws-region>.amazonaws.com/<bucket>/<key>
<bucket> is bucket name, <aws-region> is the aws region name, <key> is the object name (the value is the content of the file)
Use Restful API to access the S3 bucket
S3 bucket name must be unique gobally
S3 bucket is located in a selected region; that is your data is in a specify region
No Hierarchy for objects within a bucket; you should included a folder like structure in the name (key) of the file; eg animals/dog.txt (animals/dog is the name/key of the file); all the object in S3 are in the same level
Bucket can't inside the other bucket. 
S3 is public facing, you can access it through internet; you can use S3 gateway endpoint in VPC to make a secure connect to S3 (just like intranet)

There are many storage class in S3 (standard, intelligent tiering, stardard-IA, One Zone-IA, Glacier, Glacier Deep Archive)
Standard - default
intelligent tiering - move data for you between stroage classes based on access patterns
standard IA - standard infrequently access
One zone ia - infrequently access data but it's only stored in one availability
Glacier - for archiving
Glacier deep archive - for archiving
all with eleven 9 durability - 99.999999999%
Data availability may due to network error but not the harddrive failure 
There are minimum storage during chrage for all storage class except the standard package (it may take 30 days ~ 180 days min. charge depends on differnt classes)
Retrieval fee for cold storage is expensive
S3 Storage class selection is an art between cost, data access frequency
S3 is a region wise service but the actual storage is in a specific region

[HOL] Create Amazon S3 Bucket: S3 Dashboard, create bucket -> give bucket name, select a region (public access is blocked by default, you can add select encryption for data) -> Done
You can back to S3 and upload file to the S3 bucket that you just created; after selected the file, you can speicfy the property of that file, you can select storage class here (eg. standard, Glacier, etc), click upload -> each uploaded file will have it's unique URL


============================================================================================================================================================
************************************************************************************************************************************************************
Access Control of S3: IAM policies, bucket policies and access control lists (ACL) 

IAM policies it talking about user and role; IAM policy directly apply the permissions to the principal (user, group, role), 
Bucket Policies are resource-based policies; directly attach the policy to S3 bucket;
ACL is an old (legacy) way to secure the bucket; it's recommand to use IAM or Bucket policies; ACL can attach to bucket or an object; 
Authorization Process: Decision starts at deny; explicit deny then deny; explicit allow then allow; if not specify then deny (simily speaking you must give an allow otherwise all are deny)
Remark: S3 is defualt Block Public Access, you must disable it then apply the permission (ACL, Bucket and IAM policy). Otherwise it override everything (because it's an explicitly deny)
************************************************************************************************************************************************************
============================================================================================================================================================


[HOL] Access Control List (better to use bucket and user policy) -> goto the created S3 Bucket -> Permission -> Block public access, deselect block all public access -> goto Permission, Edit Object Ownership, ACLs Enabled -> goto Permission, Edit ACLs (leave it as defualt just take a look, no need to enable anythings here)(edit this to control the overall access on S3 BUCKET LEVEL)
ACL on object level -> Goto S3 and select an object (any file maybe an image file) -> click the permission tab of the object -> Edit the permission (OBJECT LEVEL)
The other way is goto S3 select an object -> Actions, Make Public via ACL -> Done (shortcut; setting the same things as above; it grant permission in OBJECT LEVEL)
Remark: if Block Public Access is disabled, you are not allow to edit the ACL; 
Remark: Block Public Access has an option -> "Block public access to buckets and objects granted through any access control lists (ACLs)". Therefore you must disable it to allow editing the ACL.


[HOL] Bucket and User Policy 
In root account, create a bucket and upload some images (don't set any permission at this moment)
In root account setup user accounts (username = ryan); use this non S3 permission account and login to the console, you will see that no permisson to access the S3 bucket
In root account, back to user account, select the user ryan, add permissions (you can choose "add inline policy" and edit the json)
Now use ryan to login to the console and reload the S3 bucket, you will find that the S3 dashboard is accessible by ryan (CRUD actions allowed depends on the S3 permission you grant to ryan)
In root account, goto S3, select a bucket, Permissions (no need to care about Block public access, just let it to be disable public access), edit the Bucket Policy 


[HOL] My Lab on Bucket Policy
Create a new S3 Bucket, upload some images (no permission set at this moment)
Goto the S3 Bucket, Permissions, disable "Block public access"; THE BLOCK PUBLIC ACCESS IS THE BIG DOOR TO BLOCK EVERYTHING; you must remove it otherwise even ACL, Use Policy and Bucket Policy allowed, the Block Public Access will reject everything because it's explicitly tell S3 to deny
Next goto the S3 Bucket, Permissions, Edit the Bucket Policy with the following (ryan-98760000 is the bucket name, plase update it accordingly):
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::ryan-98760000/*"
        }
    ]
}
After that everything inside the bucket is publicly available.


S3 versioning, replication, and lifecycle
Versioning - s3 keep multiple version of files, enable bucket to recover the data
Replication - Two form: cross regions replication and same region replication (both theory are the same; but one in different region and one in the same region); replication must go with versioning
Life Cycle Management: Transition actions - define when objects transition to another storage class (eg. standard, glacier); Expiration actions - define when objects expire (deleted by S3)


[HOL] Versioning and Replication
> Versioning:
Goto S3 and select a bucket -> Properties -> Bucket Versioning, enable it -> done
Upload a document (eg. xxx.txt) to the versioning enabled S3
Then update the xxx.txt content and save it, upload the updated xxx.txt to S3 again
Back to the bucket dashboard, there is a switch button “show versions”, you click click around and see the difference
Create the other file yyy.txt, upload it to S3, then delete the yyy.txt, click the switch button “show versions”, you will see that the deleted file still there due to the versioning
> Replication:
Create the other S3 bucket, Enable the bucket versioning (must set for replication)
Back to the original S3 bucket and create the replication rules, click Management tab, Replication rules, create replication rule -> give a name, enable the status, apply the rule to all object, choose the backup bucket,  IAM roles chooses create new role, you may change the destination storage class (eg. standard, glacier) -> done for replication rules
Back to the original S3 bucket and check for the newly created replication rules
Goto the newly created S3 bucket, you can’t see any thing replicated at this moment because it won’t replicate object that already exist, it only replicate newly added object
Goto the original bucket and upload, after a while (maybe few minutes for sync) the new object will be in the newly created S3 bucket
If you go back to the original bucket and delete the replication rule, then it won’t be update to the newly created S3 bucket anymore


[HOL] Lifecycle Rules: Goto S3 and select the bucket and select the Management Tab -> Create a life cycle rule -> give a name, choose the rule applies to all objects in the bucket, select the corresponding lifecycle rule actions (this example is transition current versions of objects between storage classes), set transit to Standard-IA after 30 days and move to Glacier after 90 days , take a look on the Timeline Summary -> Create Rule -> Done
You can create more rules at the same time. Eg. Expire and you set the rule of “Lifecycle rule actions” as “Permanently delete previous versions of objects”, remove after 30 days, 
After the rule is created, it is default enable, you can disable it under the lifecycle configuration of S3 

MFA on S3 for “changing the versioning state of a bucket” and “permanently delete the object”. (Versioning is needed for MFA)
The other use of MFA is called MFA Protected API Access,


S3 Encryption: it talking about the stroage encryption but not transmission encryption, transmission encryption rely on https; only the newly added file can be encrypted; There are several ways of encrytpion: server side, client side.


[HOL] Enforce Encryption: 
- Create S3 Bucket, give a name, no need to enable the encryption; just crate the bucket 
- Upload a file to the created S3 Bucket, in the file upload page goto Properties, Server-side encryption settings, select Specify an encryption key, select SSE-S3; back to the uploaded file and check then server-side encryption settings [it's the file level encryption]
- Now go back to the bucket, goto default encryption, click the edit button, and enable the server side encryption and select SSE-KMS, AWS managed key (was/s3)
- Now upload a file again without specify the encryption properties; after upload check the encryption properties of the file, you will find that it's encrypted 
- The bucket encryption setting can also be achieved by bucket policy "x-amz-server-side-encryption"


[HOL] S3 Event Notifications: something happen in S3 (eg upload) can trigger an event; sends notifications when events happen in buckets; notifications can send to SNS, SQS, Lambda
1. Create an SNS: goto then SNS dashboard, create a topic, select standard type, edit the access policy (there is a JSON template, you can copy it and edit the bucket id); after create then SNS topic, create a subscription for this topic, select email as the protocol, endpoint put the email address; goto the email account to confirm the subscription
2. Back to S3, click the Properties tab, event notifications and click then create event notification button, then give the notification a name, event types (eg. Upload a file), set the destination as SNS topic, associate it with the SNS just created
3. Back to then S3 bucket and upload a file, you should now got then notification email


[HOL] S3 Presigned URLs: allow user access an object for a limited period of time; use for those who don't have the AWS account (for those don't have the access right)
Presigned in CLI: aws s3 presign s3://bucket-name/object-name (default 1 hour)
goto bucket, upload a file; by default it's not accessible publicly; then you need to goto the AWS CLI to run the following command
aws s3 presign s3://bucket-name/object-name --expires-in 60 (it is in 60 seconds); this will generated a pre signed URL
Remark: the bucket at this moment can be completed blocked for public; but with the pre signed URL, it can be browsed by the unauthorised users
*** OR you can goto the bucket dashboard, select the file, Actions, Share with a presigned URL


Multipart Upload : uploads objects by parts and upload parallels in any order; performed by using S3 Multipart upload API
Transfer Acceleration: use cloud front edge locations to improve performance of transfer from client to S3 bucket (talking about upload from the client, but not getting data from bucket)


S3 Select is an SQL expression to retrieve individual file from zip archive (instead of getting the whole large zip file)


Server access logging can log the events that happen in S3; goto bucket and enable the logging

[HOL] S3 Static website: bucket name must match with then domain name; got that bucket click upload, add files (eg. index.html), after upload goto bucket and click the properties tab, edit the static website hosting (enable the bucket to be static website), specify the entry point (eg. index.html); 
Back to the bucket and click permissions tab; Block public access (disable then block all public access); then goto each file and select Actions, Make Public (make each file to be public)
Goto Route 53, hosted zones, select the domain name (which is the same as the bucket name), Create record with Record type = A, Route traffic to alias = alias to s3 website endpoint, select then region and endpoint


Cross-Origin Resource Sharing (CORS): must allow request form the other origin (one browser from two origins)


